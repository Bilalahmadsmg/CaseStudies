{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    " -  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "    considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail that match CHAR\n",
    "\n",
    "\n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e.\n",
    "average length of uninterrupted sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type\n",
    "`capital run length longest` i.e. length of longest uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length total` i.e.\n",
    "sum of length of uninterrupted sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data  stored in `path` using `.read_csv()` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('email_data.csv',header=None)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview of your data by using info() and describe() functions of pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=101, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9319333816075308\n",
      "Confusion Matrix: \n",
      " [[770  34]\n",
      " [ 60 517]]\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.94      0.90      0.92       577\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Copy dataset df into df1 variable and apply correlation on df1\n",
    "\n",
    "### As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other(i.e Multicollinearity), So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: \n",
      "[33, 39]\n"
     ]
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "# Remove Correlated features above 0.75 and then apply logistic model\n",
    "corr_matrix = df1.drop(57,1).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "print(\"Columns to be dropped: \")\n",
    "print(to_drop)\n",
    "df1.drop(to_drop,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=101, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df1.iloc[:,:-1]\n",
    "y = df1.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9304851556842868\n",
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 60 517]]\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.93      0.90      0.92       577\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  After keeping highly correlated features, there is not much change in the score. Lets apply another feature selection technique(Chi Squared test) to see whether we can increase our score. Find the optimum number of features using Chi Square and fit the logistic model on train data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9036929761042722\n",
      "For no of features= 25 , score= 0.9152787834902245\n",
      "For no of features= 30 , score= 0.9131064446053584\n",
      "For no of features= 35 , score= 0.9196234612599565\n",
      "For no of features= 40 , score= 0.9254163649529327\n",
      "For no of features= 50 , score= 0.9283128167994207\n",
      "For no of features= 55 , score= 0.9304851556842868\n",
      "High Score is: 0.9304851556842868 with features= 55\n"
     ]
    }
   ],
   "source": [
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=chi2 , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=101)\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Find out the accuracy , print out the Confusion Matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 60 517]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chi squared test there is no change in the score and the optimum features that we got is 55. Now lets see if we can increase our score using another feature selection technique called Anova.Find the optimum number of features using Anova and fit the logistic model on train data.\n",
    "\n",
    "### Find out the accuracy , print out the Confusion Matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.889210716871832\n",
      "For no of features= 25 , score= 0.9029688631426502\n",
      "For no of features= 30 , score= 0.9145546705286025\n",
      "For no of features= 35 , score= 0.9203475742215785\n",
      "For no of features= 40 , score= 0.9225199131064447\n",
      "For no of features= 50 , score= 0.9261404779145547\n",
      "For no of features= 55 , score= 0.9304851556842868\n",
      "High Score is: 0.9304851556842868 with features= 55\n",
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 60 517]]\n"
     ]
    }
   ],
   "source": [
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=f_classif , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n",
    "\n",
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unfortunately Anova also couldn't give us a better score . Let's finally attempt PCA on train data and find if it helps in  giving a better model by reducing the features.\n",
    "\n",
    "### Find out the accuracy , print out the Confusion Matrix.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9000724112961622\n",
      "For no of features= 25 , score= 0.9000724112961622\n",
      "For no of features= 30 , score= 0.9044170890658942\n",
      "For no of features= 35 , score= 0.9196234612599565\n",
      "For no of features= 40 , score= 0.9196234612599565\n",
      "For no of features= 50 , score= 0.9232440260680667\n",
      "For no of features= 55 , score= 0.9167270094134685\n",
      "High Score is: 0.9232440260680667 with features= 50\n",
      "Confusion Matrix: \n",
      " [[124 680]\n",
      " [239 338]]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    logistic = LogisticRegression(solver = 'lbfgs')\n",
    "    logistic.fit(X_train, y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", logistic.score(X_test,y_test))\n",
    "    \n",
    "    if logistic.score(X_test,y_test)>high_score:\n",
    "        high_score=logistic.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n",
    "\n",
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  You can also compare your predicted values and observed values by printing out values of logistic.predict(X_test[ ]) and  y_test[ ].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 10 observation:     [0 0 0 0 0 0 0 0 0 0]\n",
      "Actual values for 10 observation:  [0 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Compare observed value and Predicted value\n",
    "print(\"Prediction for 10 observation:    \",logistic.predict(X_test[0:10]))\n",
    "print(\"Actual values for 10 observation: \",y_test[0:10].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
